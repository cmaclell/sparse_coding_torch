{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ddd5bf-7615-45a2-bd9c-9827e7c2b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from conv_sparse_model import ConvSparseLayer\n",
    "from small_data_classifier import SmallDataClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train_conv3d_sparse_model import load_balls_data\n",
    "from train_conv3d_sparse_model import plot_original_vs_recon\n",
    "from train_conv3d_sparse_model import plot_filters\n",
    "from train_conv3d_sparse_model import plot_video\n",
    "\n",
    "from load_data import load_bamc_data\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86142a5-930b-4a0c-ab7a-6be63a1d1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 4*3\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, test_loader = load_bamc_data(batch_size)\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=16,\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "model = sparse_layer\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(sparse_layer.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(sparse_layer.parameters(),\n",
    "#                            momentum=0.9,\n",
    "#                            lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-14, max_lr=1e-5,step_size_up=20,mode=\"triangular2\")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d00195-42c4-4e29-9a9e-ada194cc6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Put everything on the target device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca61d5d-4f75-4409-b860-5d453b17e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d607af8e-7dd1-4bb9-9578-95c279465f11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found better model\n",
      "epoch=0, epoch_loss=3496.10, time=142.42\n",
      "found better model\n",
      "epoch=1, epoch_loss=1632.25, time=68.34\n",
      "found better model\n",
      "epoch=2, epoch_loss=1284.32, time=68.16\n",
      "found better model\n",
      "epoch=3, epoch_loss=1120.71, time=68.04\n",
      "found better model\n",
      "epoch=4, epoch_loss=1017.47, time=67.79\n",
      "found better model\n",
      "epoch=5, epoch_loss=949.60, time=67.87\n",
      "found better model\n",
      "epoch=6, epoch_loss=892.92, time=67.86\n",
      "found better model\n",
      "epoch=7, epoch_loss=848.22, time=67.97\n",
      "found better model\n",
      "epoch=8, epoch_loss=809.20, time=67.97\n",
      "found better model\n",
      "epoch=9, epoch_loss=775.57, time=67.80\n",
      "found better model\n",
      "epoch=10, epoch_loss=747.54, time=67.81\n",
      "found better model\n",
      "epoch=11, epoch_loss=723.20, time=67.91\n",
      "found better model\n",
      "epoch=12, epoch_loss=701.82, time=67.97\n",
      "found better model\n",
      "epoch=13, epoch_loss=684.20, time=67.87\n",
      "epoch=14, epoch_loss=687.83, time=67.98\n",
      "epoch=15, epoch_loss=754.30, time=68.06\n",
      "epoch=16, epoch_loss=897.27, time=68.02\n",
      "epoch=17, epoch_loss=1098.27, time=67.79\n",
      "epoch=18, epoch_loss=1264.46, time=67.84\n",
      "epoch=19, epoch_loss=1312.61, time=67.92\n",
      "epoch=20, epoch_loss=1238.64, time=67.97\n",
      "epoch=21, epoch_loss=1224.00, time=67.90\n",
      "epoch=22, epoch_loss=1163.40, time=67.81\n",
      "epoch=23, epoch_loss=1210.18, time=68.01\n",
      "epoch=24, epoch_loss=1015.56, time=67.94\n",
      "epoch=25, epoch_loss=854.14, time=67.94\n",
      "epoch=26, epoch_loss=759.18, time=68.05\n",
      "epoch=27, epoch_loss=715.80, time=67.87\n",
      "epoch=28, epoch_loss=685.29, time=68.03\n",
      "epoch=29, epoch_loss=710.14, time=68.01\n",
      "epoch=30, epoch_loss=715.45, time=67.94\n",
      "epoch=31, epoch_loss=723.78, time=67.81\n",
      "epoch=32, epoch_loss=706.07, time=67.86\n",
      "epoch=33, epoch_loss=821.11, time=67.96\n",
      "epoch=34, epoch_loss=781.28, time=67.97\n",
      "epoch=35, epoch_loss=691.34, time=67.98\n",
      "epoch=36, epoch_loss=706.95, time=67.83\n",
      "epoch=37, epoch_loss=782.16, time=67.84\n",
      "epoch=38, epoch_loss=890.65, time=67.85\n",
      "epoch=39, epoch_loss=1113.57, time=67.96\n",
      "epoch=40, epoch_loss=1273.09, time=68.04\n",
      "epoch=41, epoch_loss=1243.37, time=67.77\n",
      "epoch=42, epoch_loss=899.95, time=67.82\n",
      "found better model\n",
      "epoch=43, epoch_loss=653.51, time=67.89\n",
      "found better model\n",
      "epoch=44, epoch_loss=577.62, time=67.85\n",
      "found better model\n",
      "epoch=45, epoch_loss=545.25, time=67.97\n",
      "found better model\n",
      "epoch=46, epoch_loss=533.25, time=67.88\n",
      "epoch=47, epoch_loss=534.69, time=67.94\n",
      "epoch=48, epoch_loss=544.89, time=68.04\n",
      "epoch=49, epoch_loss=561.90, time=67.91\n",
      "epoch=50, epoch_loss=570.49, time=67.87\n",
      "epoch=51, epoch_loss=588.63, time=68.01\n",
      "epoch=52, epoch_loss=640.57, time=67.94\n",
      "epoch=53, epoch_loss=717.79, time=67.86\n",
      "epoch=54, epoch_loss=797.12, time=67.89\n",
      "epoch=55, epoch_loss=838.94, time=67.80\n",
      "epoch=56, epoch_loss=772.07, time=67.97\n",
      "epoch=57, epoch_loss=786.36, time=67.93\n",
      "epoch=58, epoch_loss=807.12, time=67.90\n",
      "epoch=59, epoch_loss=847.04, time=67.99\n",
      "epoch=60, epoch_loss=818.30, time=67.84\n",
      "epoch=61, epoch_loss=813.35, time=67.96\n",
      "epoch=62, epoch_loss=761.24, time=67.84\n",
      "epoch=63, epoch_loss=751.93, time=67.92\n",
      "epoch=64, epoch_loss=751.09, time=67.96\n",
      "epoch=65, epoch_loss=756.24, time=68.04\n",
      "epoch=66, epoch_loss=764.92, time=67.91\n",
      "epoch=67, epoch_loss=785.12, time=68.03\n",
      "epoch=68, epoch_loss=782.69, time=67.80\n",
      "epoch=69, epoch_loss=806.59, time=67.97\n",
      "epoch=70, epoch_loss=838.95, time=68.01\n",
      "epoch=71, epoch_loss=733.57, time=67.95\n",
      "epoch=72, epoch_loss=697.62, time=67.93\n",
      "epoch=73, epoch_loss=730.12, time=67.89\n",
      "epoch=74, epoch_loss=790.17, time=67.83\n",
      "epoch=75, epoch_loss=729.56, time=67.93\n",
      "epoch=76, epoch_loss=689.25, time=67.94\n",
      "epoch=77, epoch_loss=699.33, time=67.80\n",
      "epoch=78, epoch_loss=759.90, time=67.96\n",
      "epoch=79, epoch_loss=847.07, time=67.82\n",
      "epoch=80, epoch_loss=818.84, time=67.82\n",
      "epoch=81, epoch_loss=751.42, time=67.96\n",
      "epoch=82, epoch_loss=723.93, time=67.92\n",
      "epoch=83, epoch_loss=726.82, time=67.92\n",
      "epoch=84, epoch_loss=782.65, time=67.99\n",
      "epoch=85, epoch_loss=857.73, time=67.92\n",
      "epoch=86, epoch_loss=776.06, time=64.21\n",
      "epoch=87, epoch_loss=713.12, time=64.03\n",
      "epoch=88, epoch_loss=747.40, time=68.02\n",
      "epoch=89, epoch_loss=883.08, time=64.27\n",
      "epoch=90, epoch_loss=1128.19, time=64.41\n",
      "epoch=91, epoch_loss=1147.54, time=66.57\n",
      "epoch=92, epoch_loss=1120.55, time=62.90\n",
      "epoch=93, epoch_loss=1007.74, time=64.03\n",
      "epoch=94, epoch_loss=823.43, time=65.30\n",
      "epoch=95, epoch_loss=666.94, time=66.93\n",
      "epoch=96, epoch_loss=591.54, time=64.11\n",
      "epoch=97, epoch_loss=617.63, time=68.02\n",
      "epoch=98, epoch_loss=622.80, time=64.00\n",
      "epoch=99, epoch_loss=577.69, time=63.70\n",
      "epoch=100, epoch_loss=578.42, time=63.99\n",
      "epoch=101, epoch_loss=606.99, time=68.08\n",
      "epoch=102, epoch_loss=618.60, time=63.75\n",
      "epoch=103, epoch_loss=610.12, time=63.78\n",
      "epoch=104, epoch_loss=615.46, time=63.48\n",
      "epoch=105, epoch_loss=642.09, time=63.25\n",
      "epoch=106, epoch_loss=645.13, time=63.87\n",
      "epoch=107, epoch_loss=646.01, time=64.37\n",
      "epoch=108, epoch_loss=642.41, time=63.63\n",
      "epoch=109, epoch_loss=625.44, time=64.80\n",
      "epoch=110, epoch_loss=642.43, time=63.00\n",
      "epoch=111, epoch_loss=629.46, time=60.93\n",
      "epoch=112, epoch_loss=645.01, time=64.14\n",
      "epoch=113, epoch_loss=697.57, time=64.27\n",
      "epoch=114, epoch_loss=676.45, time=61.01\n",
      "epoch=115, epoch_loss=582.81, time=65.25\n",
      "epoch=116, epoch_loss=560.21, time=63.15\n",
      "epoch=117, epoch_loss=573.03, time=63.96\n",
      "epoch=118, epoch_loss=586.91, time=64.99\n",
      "epoch=119, epoch_loss=634.83, time=66.32\n",
      "epoch=120, epoch_loss=682.99, time=63.98\n",
      "epoch=121, epoch_loss=614.88, time=64.13\n",
      "epoch=122, epoch_loss=629.01, time=67.92\n",
      "epoch=123, epoch_loss=650.32, time=64.32\n",
      "epoch=124, epoch_loss=658.84, time=63.91\n",
      "epoch=125, epoch_loss=662.45, time=63.99\n",
      "epoch=126, epoch_loss=680.89, time=64.39\n",
      "found better model\n",
      "epoch=127, epoch_loss=519.08, time=65.20\n",
      "found better model\n",
      "epoch=128, epoch_loss=479.51, time=67.41\n",
      "epoch=129, epoch_loss=495.34, time=64.06\n",
      "epoch=130, epoch_loss=500.45, time=63.95\n",
      "epoch=131, epoch_loss=496.92, time=64.32\n",
      "epoch=132, epoch_loss=489.08, time=65.94\n",
      "epoch=133, epoch_loss=480.59, time=64.24\n",
      "found better model\n",
      "epoch=134, epoch_loss=472.72, time=64.28\n",
      "found better model\n",
      "epoch=135, epoch_loss=466.13, time=68.08\n",
      "epoch=136, epoch_loss=487.63, time=64.52\n",
      "epoch=137, epoch_loss=512.48, time=64.59\n",
      "epoch=138, epoch_loss=500.01, time=67.75\n",
      "epoch=139, epoch_loss=480.86, time=63.96\n",
      "epoch=140, epoch_loss=478.85, time=64.30\n",
      "epoch=141, epoch_loss=485.43, time=64.47\n",
      "epoch=142, epoch_loss=498.04, time=66.26\n",
      "epoch=143, epoch_loss=495.61, time=67.94\n",
      "epoch=144, epoch_loss=493.98, time=64.20\n",
      "epoch=145, epoch_loss=497.60, time=64.16\n",
      "epoch=146, epoch_loss=505.53, time=64.20\n",
      "epoch=147, epoch_loss=511.48, time=67.96\n",
      "epoch=148, epoch_loss=521.29, time=64.28\n",
      "epoch=149, epoch_loss=527.38, time=66.44\n",
      "epoch=150, epoch_loss=549.61, time=65.50\n",
      "epoch=151, epoch_loss=539.00, time=64.02\n",
      "epoch=152, epoch_loss=528.48, time=65.85\n",
      "epoch=153, epoch_loss=536.21, time=64.20\n",
      "epoch=154, epoch_loss=527.86, time=64.21\n",
      "epoch=155, epoch_loss=517.21, time=67.95\n",
      "epoch=156, epoch_loss=521.43, time=64.35\n",
      "epoch=157, epoch_loss=523.68, time=63.90\n",
      "epoch=158, epoch_loss=521.30, time=67.86\n",
      "epoch=159, epoch_loss=531.06, time=64.18\n",
      "epoch=160, epoch_loss=537.84, time=63.65\n",
      "epoch=161, epoch_loss=526.09, time=67.86\n",
      "epoch=162, epoch_loss=515.77, time=63.71\n",
      "epoch=163, epoch_loss=527.65, time=64.48\n",
      "epoch=164, epoch_loss=515.49, time=64.10\n",
      "epoch=165, epoch_loss=502.12, time=64.22\n",
      "epoch=166, epoch_loss=511.33, time=67.15\n",
      "epoch=167, epoch_loss=522.61, time=64.37\n",
      "epoch=168, epoch_loss=510.14, time=63.43\n",
      "epoch=169, epoch_loss=506.11, time=63.63\n",
      "epoch=170, epoch_loss=510.42, time=63.72\n",
      "epoch=171, epoch_loss=522.10, time=63.03\n",
      "epoch=172, epoch_loss=514.61, time=64.01\n",
      "epoch=173, epoch_loss=505.24, time=67.88\n",
      "epoch=174, epoch_loss=509.47, time=64.11\n",
      "epoch=175, epoch_loss=515.07, time=64.35\n",
      "epoch=176, epoch_loss=521.72, time=67.94\n",
      "epoch=177, epoch_loss=519.01, time=61.02\n",
      "epoch=178, epoch_loss=504.36, time=64.24\n",
      "epoch=179, epoch_loss=510.65, time=66.33\n",
      "epoch=180, epoch_loss=504.54, time=65.85\n",
      "epoch=181, epoch_loss=488.79, time=64.29\n",
      "epoch=182, epoch_loss=478.84, time=63.93\n",
      "epoch=183, epoch_loss=483.09, time=63.97\n",
      "epoch=184, epoch_loss=493.49, time=67.78\n",
      "epoch=185, epoch_loss=510.12, time=64.11\n",
      "epoch=186, epoch_loss=519.97, time=64.68\n",
      "epoch=187, epoch_loss=520.28, time=65.19\n",
      "epoch=188, epoch_loss=504.80, time=66.87\n",
      "epoch=189, epoch_loss=509.40, time=64.45\n",
      "epoch=190, epoch_loss=524.62, time=67.38\n",
      "epoch=191, epoch_loss=519.43, time=70.00\n",
      "epoch=192, epoch_loss=511.93, time=70.04\n",
      "epoch=193, epoch_loss=490.08, time=70.15\n",
      "epoch=194, epoch_loss=510.75, time=70.25\n",
      "epoch=195, epoch_loss=533.56, time=70.13\n",
      "epoch=196, epoch_loss=543.64, time=70.38\n",
      "epoch=197, epoch_loss=583.10, time=69.93\n",
      "epoch=198, epoch_loss=591.45, time=70.04\n",
      "epoch=199, epoch_loss=499.41, time=70.10\n",
      "epoch=200, epoch_loss=471.70, time=70.47\n",
      "epoch=201, epoch_loss=471.10, time=70.06\n",
      "epoch=202, epoch_loss=473.28, time=70.11\n",
      "epoch=203, epoch_loss=491.92, time=70.69\n",
      "epoch=204, epoch_loss=511.15, time=70.65\n",
      "epoch=205, epoch_loss=521.79, time=70.06\n",
      "epoch=206, epoch_loss=512.79, time=70.32\n",
      "epoch=207, epoch_loss=502.53, time=70.56\n",
      "epoch=208, epoch_loss=506.49, time=70.71\n",
      "epoch=209, epoch_loss=516.15, time=70.77\n",
      "epoch=210, epoch_loss=522.44, time=70.67\n",
      "epoch=211, epoch_loss=501.94, time=69.96\n",
      "epoch=212, epoch_loss=500.46, time=67.99\n",
      "epoch=213, epoch_loss=502.53, time=67.89\n",
      "epoch=214, epoch_loss=508.83, time=67.84\n",
      "epoch=215, epoch_loss=508.01, time=67.95\n",
      "epoch=216, epoch_loss=502.18, time=67.87\n",
      "epoch=217, epoch_loss=495.02, time=67.88\n",
      "epoch=218, epoch_loss=493.18, time=67.71\n",
      "epoch=219, epoch_loss=494.26, time=67.94\n",
      "epoch=220, epoch_loss=489.32, time=67.99\n",
      "epoch=221, epoch_loss=490.65, time=67.85\n",
      "epoch=222, epoch_loss=498.93, time=67.85\n",
      "epoch=223, epoch_loss=489.77, time=67.85\n",
      "epoch=224, epoch_loss=484.91, time=67.89\n",
      "epoch=225, epoch_loss=494.76, time=67.84\n",
      "epoch=226, epoch_loss=504.59, time=67.95\n",
      "epoch=227, epoch_loss=501.31, time=68.03\n",
      "epoch=228, epoch_loss=501.18, time=67.96\n",
      "epoch=229, epoch_loss=499.76, time=67.97\n",
      "epoch=230, epoch_loss=489.55, time=68.02\n",
      "epoch=231, epoch_loss=482.97, time=67.97\n",
      "epoch=232, epoch_loss=488.41, time=69.97\n",
      "epoch=233, epoch_loss=491.02, time=70.59\n",
      "epoch=234, epoch_loss=490.21, time=70.69\n",
      "epoch=235, epoch_loss=480.25, time=70.60\n",
      "epoch=236, epoch_loss=472.17, time=70.57\n",
      "epoch=237, epoch_loss=478.18, time=70.49\n",
      "epoch=238, epoch_loss=496.49, time=70.42\n",
      "epoch=239, epoch_loss=493.49, time=70.58\n",
      "epoch=240, epoch_loss=485.05, time=70.49\n",
      "epoch=241, epoch_loss=487.35, time=70.45\n",
      "epoch=242, epoch_loss=490.86, time=70.47\n",
      "epoch=243, epoch_loss=493.50, time=70.44\n",
      "epoch=244, epoch_loss=489.32, time=70.52\n",
      "epoch=245, epoch_loss=483.44, time=70.51\n",
      "epoch=246, epoch_loss=475.43, time=70.49\n",
      "epoch=247, epoch_loss=481.82, time=70.57\n",
      "epoch=248, epoch_loss=486.38, time=70.43\n",
      "epoch=249, epoch_loss=494.17, time=70.55\n",
      "epoch=250, epoch_loss=491.97, time=70.45\n",
      "epoch=251, epoch_loss=486.18, time=70.53\n",
      "epoch=252, epoch_loss=483.64, time=70.45\n",
      "epoch=253, epoch_loss=480.72, time=70.47\n",
      "epoch=254, epoch_loss=489.78, time=70.45\n",
      "epoch=255, epoch_loss=494.08, time=70.42\n",
      "found better model\n",
      "epoch=256, epoch_loss=454.98, time=70.47\n",
      "found better model\n",
      "epoch=257, epoch_loss=449.64, time=70.45\n",
      "epoch=258, epoch_loss=460.18, time=70.54\n",
      "epoch=259, epoch_loss=478.16, time=70.46\n",
      "epoch=260, epoch_loss=471.80, time=70.60\n",
      "epoch=261, epoch_loss=485.25, time=70.52\n",
      "epoch=262, epoch_loss=475.01, time=70.39\n",
      "epoch=263, epoch_loss=478.11, time=70.42\n",
      "epoch=264, epoch_loss=492.37, time=70.36\n",
      "epoch=265, epoch_loss=495.68, time=70.63\n",
      "epoch=266, epoch_loss=488.90, time=70.40\n",
      "epoch=267, epoch_loss=479.67, time=70.45\n",
      "epoch=268, epoch_loss=478.40, time=70.57\n",
      "epoch=269, epoch_loss=481.63, time=70.41\n",
      "epoch=270, epoch_loss=487.11, time=70.57\n",
      "epoch=271, epoch_loss=484.73, time=70.47\n",
      "epoch=272, epoch_loss=481.30, time=70.48\n",
      "epoch=273, epoch_loss=472.99, time=70.44\n",
      "epoch=274, epoch_loss=477.53, time=70.51\n",
      "epoch=275, epoch_loss=474.98, time=70.42\n",
      "epoch=276, epoch_loss=475.64, time=70.63\n",
      "epoch=277, epoch_loss=498.18, time=70.55\n",
      "epoch=278, epoch_loss=506.33, time=70.70\n",
      "epoch=279, epoch_loss=502.07, time=70.31\n",
      "epoch=280, epoch_loss=484.11, time=70.46\n",
      "epoch=281, epoch_loss=479.11, time=70.32\n",
      "epoch=282, epoch_loss=489.40, time=70.37\n",
      "epoch=283, epoch_loss=489.58, time=70.38\n",
      "epoch=284, epoch_loss=475.66, time=70.27\n",
      "epoch=285, epoch_loss=477.39, time=70.35\n",
      "epoch=286, epoch_loss=495.39, time=70.45\n",
      "epoch=287, epoch_loss=496.38, time=70.43\n",
      "epoch=288, epoch_loss=487.18, time=70.62\n",
      "epoch=289, epoch_loss=465.27, time=70.50\n",
      "epoch=290, epoch_loss=464.41, time=70.51\n",
      "epoch=291, epoch_loss=468.45, time=70.48\n",
      "epoch=292, epoch_loss=459.69, time=70.74\n",
      "epoch=293, epoch_loss=456.20, time=70.57\n",
      "found better model\n",
      "epoch=294, epoch_loss=446.11, time=70.56\n",
      "epoch=295, epoch_loss=448.91, time=70.42\n",
      "epoch=296, epoch_loss=464.27, time=70.50\n",
      "epoch=297, epoch_loss=485.56, time=70.48\n",
      "epoch=298, epoch_loss=497.26, time=70.55\n",
      "epoch=299, epoch_loss=485.30, time=70.68\n"
     ]
    }
   ],
   "source": [
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] = 1e-5\n",
    "# sparse_layer.lam = 0.1\n",
    "# sparse_layer.shrink = 0.1\n",
    "\n",
    "loss_log = []\n",
    "best_so_far = float('inf')\n",
    "\n",
    "for epoch in range(300):\n",
    "    epoch_loss = 0\n",
    "    epoch_start = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        # pred, activations = model(local_batch)\n",
    "        activations = model(local_batch)\n",
    "        loss = sparse_layer.loss(local_batch, activations)\n",
    "        # loss += criterion(pred, torch_labels)\n",
    "        # print('epoch={}, loss={:.2f}, time={:.2f}'.format(epoch, loss, t2-t1))\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        # print('l:', loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sparse_layer.normalize_weights()\n",
    "    \n",
    "    epoch_end = time.perf_counter()    \n",
    "    epoch_loss /= len(train_loader.sampler)\n",
    "    \n",
    "    if epoch_loss < best_so_far:\n",
    "        print(\"found better model\")\n",
    "        # Save model parameters\n",
    "        torch.save({\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, datetime.now().strftime(\"saved_models/sparse_conv3d_model-best.pt\"))\n",
    "        best_so_far = epoch_loss\n",
    "        \n",
    "    loss_log.append(epoch_loss)\n",
    "    print('epoch={}, epoch_loss={:.2f}, time={:.2f}'.format(epoch, epoch_loss, epoch_end - epoch_start))\n",
    "    # scheduler.step(epoch_loss)\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d47461-8c5d-446f-bfe6-1b823a75e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24decdf-d7dc-4a37-8877-e85742ece6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ff08a-ae7d-4bff-8106-a624427d8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save({\n",
    "    'model_state_dict': model.module.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, datetime.now().strftime(\"saved_models/sparse_conv3d_model-%Y%m%d-%H%M%S.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38348-6bcb-4dda-8583-4e86fdf8099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][1])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37229a1d-4762-4f3f-a7c1-9b5f98ca637b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx=1\n",
    "activations = sparse_layer(example_data[1][idx:idx+1].to(device))\n",
    "reconstructions = sparse_layer.reconstructions(\n",
    "    activations).cpu().detach().numpy()\n",
    "\n",
    "ani = plot_video(reconstructions.squeeze(0))\n",
    "# ani = plot_original_vs_recon(example_data[1][idx:idx+1], reconstructions, idx=0)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2ee57-4ae4-4a55-a0cb-6b265487050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_filters(sparse_layer.filters.cpu().detach())\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2ae8a-cf00-4744-b57c-6fd0fa22735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallDataClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sparse_layer = sparse_layer\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.dropout3d = torch.nn.Dropout3d(p=0.1, inplace=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(5462100, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        activations = self.sparse_layer(x)\n",
    "        \n",
    "        # x = self.dropout3d(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(activations, 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cf8c8-db9e-4c14-b9f0-418e1893ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a frozen sparse layer then add a small data classifier on top\n",
    "frozen_sparse = ConvSparseLayer(in_channels=1,\n",
    "                                out_channels=25,\n",
    "                                kernel_size=(20, 16, 16),\n",
    "                                stride=(2, 4, 4),\n",
    "                                padding=0,\n",
    "                                convo_dim=3,\n",
    "                                rectifier=True,\n",
    "                                shrink=0.25,\n",
    "                                lam=0.25,\n",
    "                                max_activation_iter=200,\n",
    "                                activation_lr=1e-2)\n",
    "sparse_param = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "frozen_sparse.load_state_dict(sparse_param['model_state_dict'])\n",
    "        \n",
    "for param in frozen_sparse.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "predictive_model = torch.nn.DataParallel(SmallDataClassifier(frozen_sparse), device_ids=[0,1,2,3])\n",
    "predictive_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "prediction_optimizer = torch.optim.Adam(predictive_model.parameters(),\n",
    "                                        lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cb611-36c6-4c39-bd19-fab35d953df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "predictive_model.to(device)\n",
    "\n",
    "idx=3\n",
    "predictive_model(example_data[1][idx:idx+1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b91fc8-3dd6-4f55-9008-5b5a6950c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    epoch_loss = 0\n",
    "    # for local_batch in train_loader:\n",
    "    t1 = time.perf_counter()\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "        \n",
    "        pred, activations = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        # loss += frozen_sparse.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        \n",
    "        prediction_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        prediction_optimizer.step()\n",
    "        \n",
    "    t2 = time.perf_counter()\n",
    "    print('epoch={}, time={:.2f}, loss={:.2f}'.format(epoch, t2-t1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60afb31-7c80-48e7-a920-80f4cab1e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    y_h = None\n",
    "    y = None\n",
    "    \n",
    "    error = None\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in test_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "\n",
    "        \n",
    "        pred, _ = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        if error is None:\n",
    "            error = torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()\n",
    "            y_h = torch.nn.Sigmoid()(pred).round().flatten()\n",
    "            y = torch_labels.flatten()\n",
    "        else:\n",
    "            error = torch.cat((error, torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()))\n",
    "            y_h = torch.cat((y_h, torch.nn.Sigmoid()(pred).round().flatten()))\n",
    "            y = torch.cat((y, torch_labels.flatten()))\n",
    "            \n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print('loss={:.2f}, time={:.2f}'.format(loss, t2-t1))\n",
    "        \n",
    "    print(\"Overall error={:.2f}\".format(error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2caa69-f3dc-4847-8166-68d32220cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y.cpu(), y_h.cpu())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b932c9-2c72-449d-a9f2-a246e3ddd325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342754c-5175-40ec-8412-48b457dd360e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chris-py3.9.6",
   "language": "python",
   "name": "chris-py3.9.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
