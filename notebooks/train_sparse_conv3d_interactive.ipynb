{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ddd5bf-7615-45a2-bd9c-9827e7c2b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from conv_sparse_model import ConvSparseLayer\n",
    "from small_data_classifier import SmallDataClassifier\n",
    "from train_conv3d_sparse_model import load_balls_data\n",
    "from train_conv3d_sparse_model import plot_original_vs_recon\n",
    "from train_conv3d_sparse_model import plot_filters\n",
    "from train_conv3d_sparse_model import plot_video\n",
    "\n",
    "from BamcPreprocessor import BamcPreprocessor\n",
    "from video_loader import MinMaxScaler\n",
    "from video_loader import VideoGrayScaler\n",
    "from video_loader import VideoLoader\n",
    "from video_loader import VideoClipLoader\n",
    "\n",
    "from load_data import load_bamc_data\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9333eb-9984-4233-af8c-2aa24ed6b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 62\n",
    "video_path = \"/shared_data/bamc_data\"\n",
    "\n",
    "width = 350\n",
    "height = 200\n",
    "\n",
    "transforms = torchvision.transforms.Compose([VideoGrayScaler(),\n",
    "                                             MinMaxScaler(0, 255),\n",
    "                                             BamcPreprocessor(),\n",
    "                                             torchvision.transforms.Resize(size=(height, width))\n",
    "                                            ])\n",
    "dataset = VideoLoader(video_path, transform=transforms, num_frames=60)\n",
    "\n",
    "targets = dataset.get_labels()\n",
    "\n",
    "train_idx, test_idx = train_test_split(np.arange(len(targets)), test_size=0.2, shuffle=True, stratify=targets)\n",
    "\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                           # shuffle=True,\n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                # shuffle=True,\n",
    "                                                sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c7fff-98d2-4388-86ef-0f23ffa2d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from os.path import join\n",
    "from os.path import isdir\n",
    "from os.path import abspath\n",
    "\n",
    "#example_data = next(iter(test_loader))\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/shared_data/bamc_data/bamc_video_info.csv')\n",
    "data['names_trimmed'] = data['Filename']\n",
    "\n",
    "for i in range(len(data['Filename'])):\n",
    "    data['names_trimmed'][i] = data['names_trimmed'][i][:-4]\n",
    "    \n",
    "for label in [\"PTX_No_Sliding\", \"PTX_Sliding\"]:\n",
    "    for f in listdir(join(video_path, label)):\n",
    "        if isfile(join(video_path, label, f)):\n",
    "            print(f[:-10])\n",
    "            data['Filename'][data['names_trimmed'] == f[:-10]] = f\n",
    "\n",
    "data = data.drop(columns=['names_trimmed'])\n",
    "data.to_csv('/shared_data/bamc_data/bamc_video_info_corrected.csv', index=False)\n",
    "#    [(label, abspath(join(video_path, label, f)), f) for f in listdir(join(video_path, label)) if isfile(join(video_path, label, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b069e2-5399-4656-9609-8141f14f97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/shared_data/bamc_data/bamc_video_info_corrected.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a4fc1-ae5d-459c-964f-e38cd96646f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example_data[1][0, 0, 10, :, :], cmap=cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86142a5-930b-4a0c-ab7a-6be63a1d1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cpu\":\n",
    "    batch_size = 1\n",
    "else:\n",
    "    batch_size = 4*3\n",
    "    # batch_size = 3\n",
    "\n",
    "# train_loader = load_balls_data(batch_size)\n",
    "train_loader, test_loader = load_bamc_data(batch_size)\n",
    "\n",
    "example_data = next(iter(train_loader))\n",
    "\n",
    "sparse_layer = ConvSparseLayer(in_channels=1,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=(4, 16, 16),\n",
    "                               stride=2,\n",
    "                               padding=0,\n",
    "                               convo_dim=3,\n",
    "                               rectifier=True,\n",
    "                               shrink=0.05,\n",
    "                               lam=0.05,\n",
    "                               max_activation_iter=300,\n",
    "                               activation_lr=1e-2)\n",
    "model = sparse_layer\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(sparse_layer.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(sparse_layer.parameters(),\n",
    "#                            momentum=0.9,\n",
    "#                            lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-14, max_lr=1e-5,step_size_up=20,mode=\"triangular2\")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46382a40-fa06-4b6b-a3c4-c83427475cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d00195-42c4-4e29-9a9e-ada194cc6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models if we'd like to\n",
    "checkpoint = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Put everything on the target device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca61d5d-4f75-4409-b860-5d453b17e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263929a2-fa6e-4217-bbfc-f9b12986b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][2])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607af8e-7dd1-4bb9-9578-95c279465f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_log = []\n",
    "best_so_far = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(300)):\n",
    "    epoch_loss = 0\n",
    "    epoch_start = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        # pred, activations = model(local_batch)\n",
    "        activations = model(local_batch)\n",
    "        loss = sparse_layer.loss(local_batch, activations)\n",
    "        # loss += criterion(pred, torch_labels)\n",
    "        # print('epoch={}, loss={:.2f}'.format(epoch, loss))\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        # print('l:', loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sparse_layer.normalize_weights()\n",
    "    \n",
    "    epoch_end = time.perf_counter()    \n",
    "    epoch_loss /= len(train_loader.sampler)\n",
    "    \n",
    "    if epoch_loss < best_so_far:\n",
    "        print(\"found better model\")\n",
    "        # Save model parameters\n",
    "        torch.save({\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, datetime.now().strftime(\"saved_models/sparse_conv3d_model-best.pt\"))\n",
    "        best_so_far = epoch_loss\n",
    "        \n",
    "    loss_log.append(epoch_loss)\n",
    "    print('epoch={}, epoch_loss={:.2f}, time={:.2f}'.format(epoch, epoch_loss, epoch_end - epoch_start))\n",
    "    # scheduler.step(epoch_loss)\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d47461-8c5d-446f-bfe6-1b823a75e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24decdf-d7dc-4a37-8877-e85742ece6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ff08a-ae7d-4bff-8106-a624427d8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "torch.save({\n",
    "    'model_state_dict': model.module.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, datetime.now().strftime(\"saved_models/sparse_conv3d_model-%Y%m%d-%H%M%S.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef38348-6bcb-4dda-8583-4e86fdf8099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_video(example_data[1][2])\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37229a1d-4762-4f3f-a7c1-9b5f98ca637b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx=1\n",
    "activations = sparse_layer(example_data[1][idx:idx+1].to(device))\n",
    "reconstructions = sparse_layer.reconstructions(\n",
    "    activations).cpu().detach().numpy()\n",
    "\n",
    "ani = plot_video(reconstructions.squeeze(0))\n",
    "# ani = plot_original_vs_recon(example_data[1][idx:idx+1], reconstructions, idx=0)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2ee57-4ae4-4a55-a0cb-6b265487050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = plot_filters(sparse_layer.filters.cpu().detach())\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2ae8a-cf00-4744-b57c-6fd0fa22735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallDataClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sparse_layer = sparse_layer\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.dropout3d = torch.nn.Dropout3d(p=0.1, inplace=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(5462100, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        activations = self.sparse_layer(x)\n",
    "        \n",
    "        # x = self.dropout3d(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(activations, 1)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490cf8c8-db9e-4c14-b9f0-418e1893ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a frozen sparse layer then add a small data classifier on top\n",
    "frozen_sparse = ConvSparseLayer(in_channels=1,\n",
    "                                out_channels=25,\n",
    "                                kernel_size=(20, 16, 16),\n",
    "                                stride=(2, 4, 4),\n",
    "                                padding=0,\n",
    "                                convo_dim=3,\n",
    "                                rectifier=True,\n",
    "                                shrink=0.25,\n",
    "                                lam=0.25,\n",
    "                                max_activation_iter=200,\n",
    "                                activation_lr=1e-2)\n",
    "sparse_param = torch.load(\"saved_models/sparse_conv3d_model-best.pt\")\n",
    "frozen_sparse.load_state_dict(sparse_param['model_state_dict'])\n",
    "        \n",
    "for param in frozen_sparse.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "predictive_model = torch.nn.DataParallel(SmallDataClassifier(frozen_sparse), device_ids=[0,1,2,3])\n",
    "predictive_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "prediction_optimizer = torch.optim.Adam(predictive_model.parameters(),\n",
    "                                        lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cb611-36c6-4c39-bd19-fab35d953df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "predictive_model.to(device)\n",
    "\n",
    "idx=3\n",
    "predictive_model(example_data[1][idx:idx+1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b91fc8-3dd6-4f55-9008-5b5a6950c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    epoch_loss = 0\n",
    "    # for local_batch in train_loader:\n",
    "    t1 = time.perf_counter()\n",
    "    for labels, local_batch in train_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "        \n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "        \n",
    "        pred, activations = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        # loss += frozen_sparse.loss(local_batch, activations)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "        \n",
    "        prediction_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        prediction_optimizer.step()\n",
    "        \n",
    "    t2 = time.perf_counter()\n",
    "    print('epoch={}, time={:.2f}, loss={:.2f}'.format(epoch, t2-t1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60afb31-7c80-48e7-a920-80f4cab1e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    y_h = None\n",
    "    y = None\n",
    "    \n",
    "    error = None\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    # for local_batch in train_loader:\n",
    "    for labels, local_batch in test_loader:\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        torch_labels = torch.zeros(len(labels))\n",
    "        torch_labels[[i for i in range(len(labels)) if labels[i] == 'PTX_No_Sliding']] = 1\n",
    "        torch_labels = torch_labels.unsqueeze(1).to(device)\n",
    "\n",
    "        \n",
    "        pred, _ = predictive_model(local_batch)\n",
    "        \n",
    "        loss = criterion(pred, torch_labels)\n",
    "        epoch_loss += loss.item() * local_batch.size(0)\n",
    "\n",
    "        if error is None:\n",
    "            error = torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()\n",
    "            y_h = torch.nn.Sigmoid()(pred).round().flatten()\n",
    "            y = torch_labels.flatten()\n",
    "        else:\n",
    "            error = torch.cat((error, torch.abs(torch_labels - torch.nn.Sigmoid()(pred).round()).flatten()))\n",
    "            y_h = torch.cat((y_h, torch.nn.Sigmoid()(pred).round().flatten()))\n",
    "            y = torch.cat((y, torch_labels.flatten()))\n",
    "            \n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print('loss={:.2f}, time={:.2f}'.format(loss, t2-t1))\n",
    "        \n",
    "    print(\"Overall error={:.2f}\".format(error.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2caa69-f3dc-4847-8166-68d32220cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y.cpu(), y_h.cpu())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b932c9-2c72-449d-a9f2-a246e3ddd325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342754c-5175-40ec-8412-48b457dd360e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chris-py3.9.6",
   "language": "python",
   "name": "chris-py3.9.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
